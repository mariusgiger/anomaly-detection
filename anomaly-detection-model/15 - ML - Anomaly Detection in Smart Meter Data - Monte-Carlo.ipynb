{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Description\n",
    "\n",
    "Energy consumption in buildings and industry is often wasted due to user behaviour, human error, and poorly performing equipment. In this context, identifying abnormal consumption power behavior can be an important part of reducing peak energy consumption and changing undesirable user behavior. With the widespread rollouts of smart meters, normal operating consumption can be learned over time and used to identify or flag abnormal consumption. Such information can help indicate to users when their equipment is not operating as normal and can help to change user behavior or to even indicate what the problem appliances may be to implement lasting changes.\n",
    "\n",
    "This challenge is looking for data scientists to apply their skills to an anomaly detection problem using smart meter data. Ideally, such an algorithm should begin to operate after as little as 3 months and should improve over time. A platform to visualise the anomalies would also be useful. Users can select any type of machine learning algorithms that they wish to in order to detect the anomalies from the data.\n",
    "\n",
    "### Data\n",
    "A sample including smart meter data can be found on [kaggle](https://www.kaggle.com/portiamurray/anomaly-detection-smart-meter-data-sample). Participants are encouraged to find other smart meter data to work with in order to test their algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import collections as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load file to 'data' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                      reading\n",
       "timestamp                   \n",
       "2016-01-01 00:15:00     2.85\n",
       "2016-01-01 00:30:00     2.85\n",
       "2016-01-01 00:45:00     3.00\n",
       "2016-01-01 01:00:00     2.94\n",
       "2016-01-01 01:15:00     2.79\n",
       "...                      ...\n",
       "2017-05-10 23:00:00     3.60\n",
       "2017-05-10 23:15:00     3.51\n",
       "2017-05-10 23:30:00     3.60\n",
       "2017-05-10 23:45:00     3.51\n",
       "2017-05-11 00:00:00     3.57\n",
       "\n",
       "[47581 rows x 1 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('SmartMeterSample.xlsx')\n",
    "data.columns = ['timestamp', 'reading']\n",
    "data = data.set_index('timestamp')\n",
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data.index.duplicated(keep='first')] # there are 4 entries with the same timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reading    3.57\n",
       "Name: 2017-05-11 00:00:00, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to with timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 3, 31, 0, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to get a timestamp from string\n",
    "datetime.fromisoformat('2016-01-01 00:15:00')\n",
    "# how to add 90 days to a specific time\n",
    "datetime.fromisoformat('2016-01-01 00:15:00') + timedelta(days=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47615\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def datetime_range(start, end, delta):\n",
    "    current = start\n",
    "    while current < end:\n",
    "        yield current\n",
    "        current += delta\n",
    "# generate timestamps for the whole period: starting on 01.01.2016, ending on 10.05.2017 \n",
    "dt = [dt for dt in \n",
    "       datetime_range(datetime(2016, 1, 1, 0, 15), datetime(2017, 5, 10, 23, 55), \n",
    "       timedelta(minutes=15))]\n",
    "\n",
    "print(len(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reading</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2016-01-01 00:15:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-01-01 00:30:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-01-01 00:45:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-01-01 01:15:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     reading\n",
       "timestamp                   \n",
       "2016-01-01 00:15:00      NaN\n",
       "2016-01-01 00:30:00      NaN\n",
       "2016-01-01 00:45:00      NaN\n",
       "2016-01-01 01:00:00      NaN\n",
       "2016-01-01 01:15:00      NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dt)\n",
    "df.columns = ['timestamp']\n",
    "df['reading'] = np.nan\n",
    "df = df.set_index('timestamp')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value at  2016-03-27 02:00:00\n",
      "Missing value at  2016-03-27 02:15:00\n",
      "Missing value at  2016-03-27 02:30:00\n",
      "Missing value at  2016-03-27 02:45:00\n",
      "Missing value at  2016-11-10 01:45:00\n",
      "Missing value at  2017-02-24 00:00:00\n",
      "Missing value at  2017-02-24 00:15:00\n",
      "Missing value at  2017-02-24 00:30:00\n",
      "Missing value at  2017-02-24 00:45:00\n",
      "Missing value at  2017-02-24 01:00:00\n",
      "Missing value at  2017-02-24 01:15:00\n",
      "Missing value at  2017-02-24 01:30:00\n",
      "Missing value at  2017-02-24 01:45:00\n",
      "Missing value at  2017-02-24 02:00:00\n",
      "Missing value at  2017-02-24 02:15:00\n",
      "Missing value at  2017-02-24 02:30:00\n",
      "Missing value at  2017-02-24 02:45:00\n",
      "Missing value at  2017-02-24 03:00:00\n",
      "Missing value at  2017-02-24 03:15:00\n",
      "Missing value at  2017-02-24 03:30:00\n",
      "Missing value at  2017-02-24 03:45:00\n",
      "Missing value at  2017-02-24 04:00:00\n",
      "Missing value at  2017-02-24 04:15:00\n",
      "Missing value at  2017-02-24 04:30:00\n",
      "Missing value at  2017-02-24 04:45:00\n",
      "Missing value at  2017-02-24 05:00:00\n",
      "Missing value at  2017-02-24 05:15:00\n",
      "Missing value at  2017-02-24 05:30:00\n",
      "Missing value at  2017-02-24 05:45:00\n",
      "Missing value at  2017-02-24 06:00:00\n",
      "Missing value at  2017-02-24 06:15:00\n",
      "Missing value at  2017-02-24 06:30:00\n",
      "Missing value at  2017-02-24 06:45:00\n",
      "Missing value at  2017-02-24 07:00:00\n",
      "Missing value at  2017-02-24 07:15:00\n",
      "Missing value at  2017-03-26 02:00:00\n",
      "Missing value at  2017-03-26 02:15:00\n",
      "Missing value at  2017-03-26 02:30:00\n",
      "Missing value at  2017-03-26 02:45:00\n"
     ]
    }
   ],
   "source": [
    "for index,row in df.iterrows():\n",
    "    if index in data.index:\n",
    "        df.loc[index]['reading'] = data.loc[index]['reading']\n",
    "    else:\n",
    "        print('Missing value at ', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    val = row['reading']\n",
    "    if val == np.nan:\n",
    "        print('Missing value at ', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomolous value at  2016-03-14 14:15:00\n",
      "Missing value at  2016-03-27 02:00:00\n",
      "Missing value at  2016-03-27 02:15:00\n",
      "Missing value at  2016-03-27 02:30:00\n",
      "Missing value at  2016-03-27 02:45:00\n",
      "Anomolous value at  2016-09-25 13:45:00\n",
      "Anomolous value at  2016-11-01 10:00:00\n",
      "Anomolous value at  2016-11-09 09:00:00\n",
      "Anomolous value at  2016-11-09 09:15:00\n",
      "Anomolous value at  2016-11-09 09:45:00\n",
      "Anomolous value at  2016-11-09 10:00:00\n",
      "Missing value at  2016-11-10 01:45:00\n",
      "Anomolous value at  2016-11-23 09:30:00\n",
      "Anomolous value at  2016-12-05 10:45:00\n",
      "Anomolous value at  2016-12-05 11:00:00\n",
      "Anomolous value at  2016-12-05 11:15:00\n",
      "Anomolous value at  2016-12-06 09:45:00\n",
      "Anomolous value at  2016-12-06 10:00:00\n",
      "Anomolous value at  2016-12-07 10:00:00\n",
      "Anomolous value at  2016-12-08 10:45:00\n",
      "Anomolous value at  2016-12-13 15:00:00\n",
      "Anomolous value at  2016-12-14 09:30:00\n",
      "Anomolous value at  2016-12-14 09:45:00\n",
      "Anomolous value at  2016-12-14 10:00:00\n",
      "Anomolous value at  2016-12-14 10:15:00\n",
      "Anomolous value at  2016-12-15 10:15:00\n",
      "Anomolous value at  2016-12-20 10:00:00\n",
      "Anomolous value at  2016-12-31 01:15:00\n",
      "Anomolous value at  2016-12-31 02:30:00\n",
      "Anomolous value at  2016-12-31 03:15:00\n",
      "Anomolous value at  2016-12-31 09:30:00\n",
      "Anomolous value at  2016-12-31 11:30:00\n",
      "Anomolous value at  2016-12-31 14:00:00\n",
      "Anomolous value at  2016-12-31 15:15:00\n",
      "Anomolous value at  2017-01-01 15:00:00\n",
      "Anomolous value at  2017-01-10 14:45:00\n",
      "Anomolous value at  2017-01-10 15:15:00\n",
      "Anomolous value at  2017-01-17 10:45:00\n",
      "Anomolous value at  2017-01-18 10:00:00\n",
      "Anomolous value at  2017-01-23 09:15:00\n",
      "Anomolous value at  2017-01-23 10:00:00\n",
      "Anomolous value at  2017-01-23 11:15:00\n",
      "Anomolous value at  2017-01-24 10:45:00\n",
      "Anomolous value at  2017-01-24 11:00:00\n",
      "Anomolous value at  2017-01-24 11:15:00\n",
      "Anomolous value at  2017-01-24 11:30:00\n",
      "Anomolous value at  2017-01-24 11:45:00\n",
      "Anomolous value at  2017-01-24 12:30:00\n",
      "Anomolous value at  2017-01-24 15:15:00\n",
      "Anomolous value at  2017-01-31 13:45:00\n",
      "Anomolous value at  2017-01-31 14:15:00\n",
      "Anomolous value at  2017-01-31 16:45:00\n",
      "Anomolous value at  2017-02-21 10:30:00\n",
      "Anomolous value at  2017-02-21 10:45:00\n",
      "Anomolous value at  2017-02-21 11:15:00\n",
      "Anomolous value at  2017-02-21 11:30:00\n",
      "Missing value at  2017-02-24 00:00:00\n",
      "Missing value at  2017-02-24 00:15:00\n",
      "Missing value at  2017-02-24 00:30:00\n",
      "Missing value at  2017-02-24 00:45:00\n",
      "Missing value at  2017-02-24 01:00:00\n",
      "Missing value at  2017-02-24 01:15:00\n",
      "Missing value at  2017-02-24 01:30:00\n",
      "Missing value at  2017-02-24 01:45:00\n",
      "Missing value at  2017-02-24 02:00:00\n",
      "Missing value at  2017-02-24 02:15:00\n",
      "Missing value at  2017-02-24 02:30:00\n",
      "Missing value at  2017-02-24 02:45:00\n",
      "Missing value at  2017-02-24 03:00:00\n",
      "Missing value at  2017-02-24 03:15:00\n",
      "Missing value at  2017-02-24 03:30:00\n",
      "Missing value at  2017-02-24 03:45:00\n",
      "Missing value at  2017-02-24 04:00:00\n",
      "Missing value at  2017-02-24 04:15:00\n",
      "Missing value at  2017-02-24 04:30:00\n",
      "Missing value at  2017-02-24 04:45:00\n",
      "Missing value at  2017-02-24 05:00:00\n",
      "Missing value at  2017-02-24 05:15:00\n",
      "Missing value at  2017-02-24 05:30:00\n",
      "Missing value at  2017-02-24 05:45:00\n",
      "Missing value at  2017-02-24 06:00:00\n",
      "Missing value at  2017-02-24 06:15:00\n",
      "Missing value at  2017-02-24 06:30:00\n",
      "Missing value at  2017-02-24 06:45:00\n",
      "Missing value at  2017-02-24 07:00:00\n",
      "Missing value at  2017-02-24 07:15:00\n",
      "Anomolous value at  2017-03-07 14:30:00\n",
      "Missing value at  2017-03-26 02:00:00\n",
      "Missing value at  2017-03-26 02:15:00\n",
      "Missing value at  2017-03-26 02:30:00\n",
      "Missing value at  2017-03-26 02:45:00\n",
      "Anomolous value at  2017-04-25 11:00:00\n",
      "Anomolous value at  2017-04-26 11:15:00\n"
     ]
    }
   ],
   "source": [
    "df['freq'] = 0\n",
    "appr = cl.defaultdict(float)\n",
    "count = 0\n",
    "alert_threshold = 0.0002\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    val = row['reading']\n",
    "    if pd.isna(val):\n",
    "        print('Missing value at ', index)\n",
    "    else:\n",
    "        count += 1\n",
    "        if val in appr:\n",
    "            appr[val] += 1\n",
    "        else:\n",
    "            appr[val] = 1\n",
    "        row['freq'] = appr[val]/count\n",
    "        if row['freq'] < alert_threshold:\n",
    "            print('Anomolous value at ', index)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
